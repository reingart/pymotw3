# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, Doug Hellmann
# This file is distributed under the same license as the PyMOTW-3 package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2017.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: PyMOTW-3 \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-03-24 18:41-0300\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.3.4\n"

#: ../../source/urllib.robotparser/index.rst:3
msgid "urllib.robotparser --- Internet Spider Access Control"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:8
msgid "Parse ``robots.txt`` file used to control Internet spiders"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:10
msgid ""
":mod:`robotparser` implements a parser for the ``robots.txt`` file "
"format, including a function that checks if a given user agent can access"
" a resource.  It is intended for use in well-behaved spiders, or other "
"crawler applications that need to either be throttled or otherwise "
"restricted."
msgstr ""

#: ../../source/urllib.robotparser/index.rst
#: ../../source/urllib.robotparser/index.rst:17
msgid "robots.txt"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:19
msgid ""
"The ``robots.txt`` file format is a simple text-based access control "
"system for computer programs that automatically access web resources "
"(\"spiders\", \"crawlers\", etc.).  The file is made up of records that "
"specify the user agent identifier for the program followed by a list of "
"URLs (or URL prefixes) the agent may not access."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:25
msgid "This is the ``robots.txt`` file for ``https://pymotw.com/``:"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:30
msgid ""
"It prevents access to some of the parts of the site that are expensive to"
" compute and would overload the server if a search engine tried to index "
"them.  For a more complete set of examples of ``robots.txt``, refer to "
"`The Web Robots Page`_."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:36
msgid "Testing Access Permissions"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:38
msgid ""
"Using the data presented earlier, a simple crawler can test whether it is"
" allowed to download a page using ``RobotFileParser.can_fetch()``."
msgstr ""

#: ../../source/urllib.robotparser/index.rst
msgid "urllib_robotparser_simple.py"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:45
msgid ""
"The URL argument to ``can_fetch()`` can be a path relative to the root of"
" the site, or full URL."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:72
msgid "Long-lived Spiders"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:74
msgid ""
"An application that takes a long time to process the resources it "
"downloads or that is throttled to pause between downloads should check "
"for new ``robots.txt`` files periodically based on the age of the content"
" it has downloaded already.  The age is not managed automatically, but "
"there are convenience methods to make tracking it easier."
msgstr ""

#: ../../source/urllib.robotparser/index.rst
msgid "urllib_robotparser_longlived.py"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:85
msgid ""
"This extreme example downloads a new ``robots.txt`` file if the one it "
"has is more than one second old."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:111
msgid ""
"A nicer version of the long-lived application might request the "
"modification time for the file before downloading the entire thing. On "
"the other hand, ``robots.txt`` files are usually fairly small, so it is "
"not that much more expensive to just retrieve the entire document again."
msgstr ""

#: ../../source/urllib.robotparser/index.rst:120
msgid ":pydoc:`urllib.robotparser`"
msgstr ""

#: ../../source/urllib.robotparser/index.rst:122
msgid "`The Web Robots Page`_ -- Description of ``robots.txt`` format."
msgstr ""

